{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## This notebook shows alternative derivations of the RBM equations, and provides a space for discussion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "the idea of the generative learning formulation is to add a data-dependent regularizer to training ($-log~p(\\mathbf{x^{(t)}})$)       \n",
      "_we don't need a label to compute  $-log~p(\\mathbf{x^{(t)}})$ _ "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We know the joint distribution p(x,h), it's derived from the energy function:           \n",
      "\n",
      "The Energy Function: $E(x,h) = -h^{T}Wx - c^{T}x - b^{T}h $     \n",
      "       \n",
      "$p(\\mathbf{x}, \\mathbf{h}) = exp(-E(x,h))/Z$        \n",
      "\n",
      "$ = e^{\\mathbf{h}^T \\cdot \\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{c}^T \\cdot \\mathbf{x} + \\mathbf{b}^T \\cdot \\mathbf{h} }  / Z$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"http://i.imgur.com/OA0iYmY.png\" alt=\"Basic RBM Formulation\" style=\"width: 500px;\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"http://i.imgur.com/Lq1mizF.png\" alt=\"Markov Network View\" style=\"width: 500px;\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"http://i.imgur.com/kNzvEyU.png\" alt=\"Markov Network View\" style=\"width: 500px;\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"http://i.imgur.com/mk2hk5Y.png\" alt=\"Markov Network View\" style=\"width: 500px;\"/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\"the probability of observing a particular configuration of our variable of interest\"                  \n",
      "$p(\\mathbf{x, h}) = exp(-E(\\mathbf{x,h})) / Z $\n",
      "\n",
      "> \"the exponential of the _NEGATIVE_ energy normalized by the partition function $Z$\"             \n",
      "      \n",
      "- note what each component in the energy equation does -- i.e. if $c_k$ is negative and $x_k$ is 1, we will _INCREASE_ the energy     \n",
      "- the term $-h^TWx = -\\sum\\limits_{j} \\sum\\limits_{h}W_{j,k}h_{j}x_{k}$ models the preference for _BOTH_ $h_j$ and $x_k$ to be 1\n",
      "        \n",
      "#### computing $p(\\mathbf{x, h}) = exp(-E(\\mathbf{x,h})) / Z $ is generally intractable, because Z is an exponential sum over all possible configurations of $v$ and $h$        \n",
      "* so, we need to estimate it somehow       \n",
      "     \n",
      "#### there are other inferences that _are_ tractable, however\n",
      "* notably $p(x|h)$ and $p(h|x)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$p(x|h)$ factorizes into $p(\\mathbf{x|h}) = \\prod\\limits_{k}p(\\mathbf{x_k|h}) $ because the $x_k$'s are conditionally independent given $h$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since $h_j \\in \\{0,1\\}$, ($h_j$ is a binary-valued random variable) -- we have a bernoulli with the following form:      \n",
      "$ p(h_j = 1 | x) = \\dfrac{1}{ 1 + e^{-(b_j + W_j.x)} } $       \n",
      "       \n",
      "$ = sigm(b_j + W_jx) $\n",
      "       \n",
      "Likewise:      \n",
      "$ p(x_k = 1 | h) = \\dfrac{1}{ 1 + e^{-(c_k + h^{T}W_{.k})} } $      \n",
      "      \n",
      "$ = sigm(c_k + h^{T}W_{.k}) $      \n",
      "      \n",
      "### Important - notice the $W_{.k}$ notation for $p(x_k = 1 | h) $-- this is the $k^{th}$ _COLUMN_ of $W$ \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## the derivation for $ p(h|x) $\n",
      "$ p(x) = \\sum \\limits_{h'}p(x, h') $              \n",
      "$ p(h | x) = p(h,x) / \\sum \\limits_{h'}p(x, h') $      \n",
      "> \"The joint of $h$ and $x$ divided by the marginal over all $x$\"                      \n",
      "         \n",
      " \n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ok, so let's write out what $p(h|x)$ actually is (i.e. fill in the equations for $p(x,h)$ from above:    \n",
      "\n",
      "$ = \\dfrac{exp(h^{T}Wx + c^{T}x + b^{T}h)/Z}{ \\sum\\limits_{h' \\in \\{0,1\\}^H}exp(h'^{T}Wx + c^{T}x + b^{T}h') / Z} $      \n",
      "      \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "we can cancel out the partition function $Z$ and the term $c^{T}x$ because they do not depend on $h$       \n",
      "#### to see this, note that we can write:      \n",
      "$ = \\dfrac{(exp(h^{T}Wx) * exp(c^{T}x) * exp(b^{T}h)) / Z}{ exp(c^Tx)/ Z\\sum\\limits_{h' \\in \\{0,1\\}^H}exp(h'^{T}Wx + b^{T}h')} $       \n",
      "     \n",
      "* then cancel $Z$ and $c^{T}x$ from numerator and denominator\n",
      "        \n",
      "$ = \\dfrac{exp(h^{T}Wx + b^{T}h)}{ \\sum\\limits_{h' \\in \\{0,1\\}^H}exp(h'^{T}Wx + b^{T}h')} $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### now, notice that: $ h^{T}Wx + b^{T}h = \\sum\\limits_{j}h_{j}W_{j}x + b_{j}h_{j} $       \n",
      "##### Note: $h_j$ is a scalar, $W_j$ is a row, $x$ is a column vector "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Ok, let's expand that sum in the denominator into an explicit sum over the possible values of the hidden unit     \n",
      "      \n",
      "$ = \\dfrac{exp(\\sum\\limits_{j}h_{j}W_{j}x + b_{j}h_{j})}{\\sum \\limits_{h'_0 \\in \\{0,1\\}} ... \\sum \\limits_{h'_{H} \\in \\{0,1\\}}exp(\\sum\\limits_{j}h'_{j}W_{j}x + b_{j}h'_{j})} $\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Because the exponential of a sum is the product of exponentials, we can do this:      \n",
      "           \n",
      "$ = \\dfrac{\\prod\\limits_{j}exp(h_{j}W_{j}x + b_{j}h_{j})}{\\sum \\limits_{h'_0 \\in \\{0,1\\}} ... \\sum \\limits_{h'_{H} \\in \\{0,1\\}}\\prod\\limits_{j}exp(h'_{j}W_{j}x + b_{j}h'_{j})} $\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Notice that the denominator ($ \\sum \\limits_{h'_0 \\in \\{0,1\\}} ... \\sum \\limits_{h'_{H} \\in \\{0,1\\}}\\prod\\limits_{j}exp(h'_{j}W_{j}x + b_{j}h'_{j}) $) contains an exponential number of factors...        \n",
      "       \n",
      "### ... In the next step, we'll convert that to a linear number of factors       \n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## First, notice that each element of the product: $\\prod\\limits_{j}exp(h'_{j}W_{j}x + b_{j}h'_{j}) $ only depends on the value of a single $h'_j$      \n",
      "       \n",
      "    \n",
      "## This allows us to convert an exponential nested sum into a linear product over $ H $ terms -- to see this, work through the following derivation:\n",
      "\n",
      "### Gavin's way:\n",
      "\n",
      "The $h_H$ value is now either $0$, in which case we get an exponential yeilding 1 times the remaining product and an expression depending on $h_H$ times the remaining product which we can then factorise the product back out of to obtain this:\n",
      "\n",
      "$ = \\sum \\limits_{h_0 \\in \\{0,1\\}} ... \\sum \\limits_{h_{(H-1)} \\in \\{0,1\\}} \\prod\\limits_{j=1}^{H-1} \\exp{(h_j \\mathbf{W}_j \\cdot \\mathbf{x} + b_j h_j)}\\exp{(0)} + \\prod\\limits_{j=1}^{H-1} \\exp{(h_j \\mathbf{W}_j \\cdot \\mathbf{x} + b_j h_j)}\\exp{(\\mathbf{W}_H \\cdot \\mathbf{x} + b_H})$\n",
      "\n",
      "$ = \\sum \\limits_{h_0 \\in \\{0,1\\}} ... \\sum \\limits_{h_{(H-1)} \\in \\{0,1\\}} \\prod\\limits_{j=1}^{H-1} \\exp{(h_j \\mathbf{W}_j \\cdot \\mathbf{x} + b_j h_j)} (1  + \\exp{(\\mathbf{W}_H \\cdot \\mathbf{x} + b_H}))$\n",
      "\n",
      "\n",
      "Repeat this trick for each sum from $H-1$ down to $0$.\n",
      "\n",
      "$ = (1 + \\exp{(\\mathbf{W}_1 \\cdot \\mathbf{x} + b_1})) \\times ... \\times (1  + \\exp{(\\mathbf{W}_H \\cdot \\mathbf{x} + b_H}))$\n",
      "\n",
      "$ = \\prod\\limits_{j=1}^{H} (1  + \\exp{(\\mathbf{W}_j \\cdot \\mathbf{x} + b_j}))$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, the full derivation becomes:     \n",
      "      \n",
      "$ \\dfrac{\\prod\\limits_{j}exp(h_{j}W_{j}x + b_{j}h_{j})}{\\prod\\limits_{j} (1  + \\exp{(\\mathbf{W}_j \\cdot \\mathbf{x} + b_j}))} $      \n",
      "      \n",
      "put the $\\prod$ in front of the fraction:       \n",
      "       \n",
      "\n",
      "$ = \\prod\\limits_{j}\\dfrac{exp(h_{j}W_{j}x + b_{j}h_{j})}{1  + \\exp{(\\mathbf{W}_j \\cdot \\mathbf{x} + b_j})} $ \n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, notice that, for an individual $ h_j $ the expression $ \\dfrac{exp(h_{j}W_{j}x + b_{j}h_{j})}{1  + \\exp{(\\mathbf{W}_j \\cdot \\mathbf{x} + b_j})} $ is actually a probability, because the sum over the possible values of $ h_j $ is $ = 1 $         \n",
      "       \n",
      "i.e. if $h_j$ is $= 0$, we have $ \\dfrac{1}{1  + \\exp{(\\mathbf{W}_j \\cdot \\mathbf{x} + b_j})} $ and if $h_j = 1$ we have $ \\dfrac{exp(W_{j} \\cdot x + b_{j})}{1  + \\exp{(\\mathbf{W}_j \\cdot \\mathbf{x} + b_j})} $       \n",
      "       \n",
      "so the sum of the two possible values is:      \n",
      "$ = \\dfrac{1 + exp(W_{j} \\cdot x + b_{j})}{1  + \\exp{(W_{j} \\cdot x + b_j})} = 1 $      \n",
      "       \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we conclude the following:      \n",
      "       \n",
      "$ = \\prod\\limits_{j}\\dfrac{exp(h_{j}W_{j}x + b_{j}h_{j})}{1  + \\exp{(\\mathbf{W}_j \\cdot \\mathbf{x} + b_j})} $        \n",
      "        \n",
      "which allows us to write:       \n",
      "## $ p(h|x) = \\prod\\limits_{j} p(h_j|x) $\n",
      "\n",
      "***"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### ok now let's show that: $ p(h_j = 1|x) = \\dfrac{exp(h_{j}W_{j}x + b_{j}h_{j})}{1  + \\exp{(\\mathbf{W}_j \\mathbf{x} + b_j})} $       \n",
      "       "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$ = \\dfrac{(exp(h_{j}W_{j}x + b_{j}h_{j}))(exp(-b_j - W_jx))}{1  + \\exp{(\\mathbf{W}_j \\mathbf{x} + b_j})(exp(-b_j - W_jx))} $    \n",
      "         \n",
      "$ = \\dfrac{1}{1+ exp(-b_j - W_jx)} $             \n",
      "       \n",
      "$ = sigm(b_j + W_jx) $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*** \n",
      "#### the derivations above fit into the \"LOCAL MARKOV PROPERTY\", which is true for all undirected grapical models      \n",
      "       \n",
      "<img src=\"http://i.imgur.com/B0UpC3K.png\" alt=\"Markov Network View\" style=\"width: 500px;\"/>\n",
      "***"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Now let's derive $p(x)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For $p(\\mathbf{x})$, first we take $p(\\mathbf{x},\\mathbf{h})$ and take the marginal, that is, remove dependence on $\\mathbf{h}$ by summing over all possibilities.  The partition function remains unchanged as it already sums over all possibilites of $\\mathbf{x}$ and $\\mathbf{h}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "\n",
      "\n",
      "$ p(\\mathbf{x}) = \\sum \\limits_{\\mathbf{h} \\in \\{0,1\\}^H } e^{\\mathbf{h}^T \\cdot \\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{c}^T \\cdot \\mathbf{x} + \\mathbf{b}^T \\cdot \\mathbf{h} }  / Z$ \n",
      "\n",
      "We factor out the exponential of $\\mathbf{c}^T \\cdot \\mathbf{x}$ which has no dependence on $\\mathbf{h}$ in the sum along with the normalisation factor (the partition function).\n",
      "\n",
      "$ = e^{\\mathbf{c}^T \\cdot \\mathbf{x}} / Z \\sum \\limits_{\\mathbf{h} \\in \\{0,1\\}^H} \\exp{(\\mathbf{h}^T \\cdot \\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b}^T \\cdot \\mathbf{h})}$\n",
      "\n",
      "We then make the inner product an explit sum over an index. \n",
      "\n",
      "$ = e^{\\mathbf{c}^T \\cdot \\mathbf{x}} / Z \\sum \\limits_{\\mathbf{h} \\in \\{0,1\\}^H} \\exp{(\\sum\\limits_{j=1}^H h_j \\mathbf{W}_j \\cdot \\mathbf{x} + b_j h_j)} $\n",
      "\n",
      "Exponentials of finite sums are finite products of exponentials, hence we can factorise again.\n",
      "\n",
      "$ = e^{\\mathbf{c}^T \\cdot \\mathbf{x}} / Z \\sum \\limits_{\\mathbf{h} \\in \\{0,1\\}^H} \\prod\\limits_{j=1}^H \\exp{(h_j \\mathbf{W}_j \\cdot \\mathbf{x} + b_j h_j)} $\n",
      "\n",
      "We can expand the sum over values drawing from the set $\\{0,1\\}^H$ by looping over each bit starting from the outermost.  You can imagine this as a large number of nested for-loops, one for each bit. \n",
      "\n",
      "$ = e^{\\mathbf{c}^T \\cdot \\mathbf{x}} / Z \\sum \\limits_{h_0 \\in \\{0,1\\}} ... \\sum \\limits_{h_H \\in \\{0,1\\}} \\prod\\limits_{j=1}^H \\exp{(h_j \\mathbf{W}_j \\cdot \\mathbf{x} + b_j h_j)}$\n",
      "\n",
      "The $h_H$ value is now either $0$, in which case we get an exponential yeilding 1 times the remaining product and an expression depending on $h_H$ times the remaining product which we can then factorise the product back out of to obtain this:\n",
      "\n",
      "$ = e^{\\mathbf{c}^T \\cdot \\mathbf{x}} / Z \\sum \\limits_{h_0 \\in \\{0,1\\}} ... \\sum \\limits_{h_{(H-1)} \\in \\{0,1\\}} \\prod\\limits_{j=1}^{H-1} \\exp{(h_j \\mathbf{W}_j \\cdot \\mathbf{x} + b_j h_j)}\\exp{(0)} + \\prod\\limits_{j=1}^{H-1} \\exp{(h_j \\mathbf{W}_j \\cdot \\mathbf{x} + b_j h_j)}\\exp{(\\mathbf{W}_H \\cdot \\mathbf{x} + b_H})$\n",
      "\n",
      "$ = e^{\\mathbf{c}^T \\cdot \\mathbf{x}} / Z \\sum \\limits_{h_0 \\in \\{0,1\\}} ... \\sum \\limits_{h_{(H-1)} \\in \\{0,1\\}} \\prod\\limits_{j=1}^{H-1} \\exp{(h_j \\mathbf{W}_j \\cdot \\mathbf{x} + b_j h_j)} (1  + \\exp{(\\mathbf{W}_H \\cdot \\mathbf{x} + b_H}))$\n",
      "\n",
      "\n",
      "Repeat this trick for each sum from $H-1$ down to $0$.\n",
      "\n",
      "$ = e^{\\mathbf{c}^T \\cdot \\mathbf{x}} / Z \\times (1 + \\exp{(\\mathbf{W}_1 \\cdot \\mathbf{x} + b_1})) \\times ... \\times (1  + \\exp{(\\mathbf{W}_H \\cdot \\mathbf{x} + b_H}))$\n",
      "\n",
      "$ = e^{\\mathbf{c}^T \\cdot \\mathbf{x}} / Z \\prod\\limits_{j=1}^{H} (1  + \\exp{(\\mathbf{W}_j \\cdot \\mathbf{x} + b_j}))$\n",
      "\n",
      "We can then use the fact that $\\exp{(\\log{(x)})}$ is the identity function to rewrite this as:\n",
      "\n",
      "$ = e^{\\mathbf{c}^T \\cdot \\mathbf{x}} / Z \\prod\\limits_{j=1}^H \\exp{(\\log{(1  + \\exp{(\\mathbf{W}_j \\cdot \\mathbf{x} + b_j}))})}$ \n",
      "\n",
      "Again using product of exponentials is exponential of sums we can get a probability expression in terms of a free energy:\n",
      "\n",
      "$ =\\exp{(\\mathbf{c}^T \\cdot \\mathbf{x} + \\sum\\limits_{j=1}^H \\log{(1  + \\exp{(\\mathbf{W}_j \\cdot \\mathbf{x} + b_j}))} )} / Z$ \n",
      "\n",
      "\n",
      "\n",
      "$ p(\\mathbf{h}|x) = \\prod_j p(h_j | x) $ \n",
      "\n",
      "$ p(h_j = 1 | x) = \\dfrac{1}{ 1 + e^{-(\\beta_j + W_j.x)} } $  \n",
      "$ p(x|h) $\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}